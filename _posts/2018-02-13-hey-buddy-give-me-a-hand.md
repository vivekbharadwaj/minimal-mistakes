---
title: "Hey buddy, can you give me a hand?"
excerpt: "Your friendly Spot-Mini can now open closed doors"
layout: single
header:
    overlay_image: /assets/images/spot-mini.png
    overlay_filter: 0.2
categories:
  - boston dynamics
  - robots
  - cooperation
tags:
  - robot overlords
  - cooperation
author: "Vivek"
date: "13 February 2018"
hidden: false
---

Can I just say I was equal parts thrilled and creeped out by Boston Dynamics' [latest video](https://youtu.be/fUyU3lKzoio) of Spot-Minis cooperating to open doors? 

<iframe width="560" height="315" src="https://www.youtube.com/embed/fUyU3lKzoio" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

While the entire video was quite interesting, I loved the part where one of the Spot-Minis acts as if it realises that the door is closed. Then it patiently waits for the **other** Spot-Mini with a hand-like contraption to come and open the door. Did the robots communicate, communicate and work together to open teh door and get out of the room? I strongly believe that part was entirely scripted for the purpose of the video. Still, it's not too hard to speculate that someone is working on the problem of large-scale cooperation between AI agents and human beings. 

Now, I struggled to find any published papers from Boston Dynamics about how they do what they do. In fact, notoriously little is known of the actual tech and learning process that Boston Dynamics (BD) use in their robots. I came across a reddit post some time ago where some insiders claimed that BD robots are mostly meticulously hand-engineered rather than architected to use machine learning and artificial intelligence extensively. 

Cooperation and the general feeling of helping out is is innate human behaviour and this has been shown several times in research where 18 month old babies try to "comfort others in distress, participate in household tasks, and help adults by bringing or pointing to out-of-reach objects". You can read about one such fantastic study [here](https://www.ncbi.nlm.nih.gov/pubmed/16513986/). We haven't fully understood what goes on inside our minds that makes us flexibly and instinctively cooperate. I think it's crucial we figure this out soon and work towards building empathetic AI agents and robots.

Cooperation between AI and machine learning agents is being studied by other research groups too. OpenAI recently released a paper about cooperation strategies in a multi-agent setting. Another top research group called DeepMind (they've been bought by Google) used deep multi-agent reinforcement learning models to understand complex behaviours of cooperation and competition. DeepMinds' research more closely modeled game theoritical simulations of [Prisoner's Dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma), which features interactions and strategies in a closed environment - for ex: 2 players. Interestingly, in one of their games "Gathering", AI agents became less cooperative and more aggresive when they were either resource-starved as well as when agent strategies became more complex. When you're in a social context however, you're interacting with a much larger number of people over a longer time scale and with different value functions. These are the kinds of models that  social scientists and macro-economists deal with, and would sure be interesting to model on a large scale for AI.

Cover Image courtesy: [Boston Dynamics](https://www.bostondynamics.com/spot-mini) <br>
See here for more information about the [DeepMind research](https://deepmind.com/blog/understanding-agent-cooperation/) as well as the [OpenAI research paper](https://arxiv.org/abs/1706.02275) <br>
You may also want to check out some interesting discussion about this [on reddit](https://www.reddit.com/r/mechanical_gifs/comments/7x3m3r/hey_buddy_can_you_give_me_a_hand_boston_dynamics/)